# Introduction to the Workshop

## Who this workshop is for

* This workshop is for those who want to learn about the deep features of DifferentialEquations.jl for ODEs
    * It is assumed you know what ODEs are and what they are useful for
    * We will not cover SDEs, DDEs, jump processes, etc., it's just ODEs
* We assume that you know a good amount of Julia
* This is for intermediates, not for experts (but that would be a fun workshop to give if folks are interested)

For a more beginner version, see on Youtube:

"Intro to solving differential equations in Julia"

## Note about coming changes

There will soon be a new major release, DifferentialEquations.jl v8!

* The major change will be that non-ODE dependencies will be removed from DifferentialEquations.jl
    * The documentation will still cover all types of differential equations
    * For things other than ODEs, you will be required to import the solver packages
        * SDEs: StochasticDiffEq.jl
        * DDEs: DelayDiffEq.jl
        * ...
    * The default solvers will be associated with those domain packages
* Result: DifferentialEquations.jl will
    * have much fewer dependencies
    * be much faster to load 
    * be focused on ODEs

With that in mind, let's start the show!

# Introduction to DifferentialEquations.jl

Let's do a quick runthrough of the basics!

## Defining and solving an ODE

Let's solve the Lorenz equations

$$
\begin{aligned}
\frac{dx}{dt} &= σ(y-x) \\
\frac{dy}{dt} &= x(ρ-z) - y \\
\frac{dz}{dt} &= xy - βz \\
\end{aligned}
$$

## Defining and solving an ODE

We will use the common notation. The ODE is defined as:

$$
u' = f(u,p,t)
$$

where $u$ is the state vector, $p$ are the parameters, and $t$ is the
independent variable. In this workshop we will only be looking at initial
value problems, i.e. problems for which $u_0 = u(t_0)$ is given. 

For cases where that is not true, see the Boundary Value Problem solvers
(BoundaryValueDiffEq.jl, `BVProblem`) which will have its own JuliaCon
talk

## Defining and solving an ODE

```{julia}
#| echo: true
import DifferentialEquations as DE

function lorenz!(du, u, p, t)
    x, y, z = u
    σ, ρ, β = p
    du[1] = dx = σ * (y - x)
    du[2] = dy = x * (ρ - z) - y
    du[3] = dz = x * y - β * z
end

u0 = [1.0, 0.0, 0.0]
tspan = (0.0, 100.0)
p = [10.0, 28.0, 8 / 3]
prob = DE.ODEProblem(lorenz!, u0, tspan, p)
```

## Defining and solving an ODE

```{julia}
#| echo: true
sol = DE.solve(prob)
```

## Plotting the Solution

```{julia}
import Plots
Plots.plot(sol, idxs = (1, 2, 3))
```

## Tweaking the tolerances

```{julia}
#| echo: true
sol = DE.solve(prob; abstol=1e-8, reltol=1e-8)
```

* `abstol`: controls behavior near zero
* `reltol`: general tolerance

## Interpolating the solution

```{julia}
#| echo: true
sol(2.0)
```

```{julia}
#| echo: true
sol([1.0,2.0])
```

You can even get derivatives!

```{julia}
#| echo: true
sol([1.0,2.0], Val{1}) # 1st derivative
```

## Controlling Saving

```{julia}
#| echo: true
sol = DE.solve(prob; saveat = 1.0)
```

```{julia}
#| echo: true
sol = DE.solve(prob; saveat = [1.0,5.0,100.0])
```

## Choosing Solvers

```{julia}
sol = DE.solve(prob, DE.Tsit5())
```

```{julia}
#| echo: true
sol = DE.solve(prob, DE.Vern9())
```

## What Solvers Are There?

```
OrdinaryDiffEq.jl Solvers: 299

| Category                | Count |
|-------------------------|-------|
| Adams-Bashforth-Moulton | 13    |
| BDF Methods             | 18    |
| Explicit RK             | 1     |
| Exponential RK          | 17    |
| Extrapolation           | 7     |
| Feagin                  | 3     |
| FIRK                    | 4     |
| Function Map            | 1     |
| High Order RK           | 4     |
| IMEX Multistep          | 2     |
| Linear/Magnus           | 16    |
| Low Order RK            | 26    |
| Low Storage RK          | 45    |
| Nordsieck               | 4     |
| PDIRK                   | 1     |
| PRK                     | 1     |
| QPRK                    | 1     |
| RKN (Nyström)           | 17    |
| Rosenbrock              | 37    |
| SDIRK                   | 29    |
| SSPRK                   | 13    |
| Stabilized IRK          | 1     |
| Stabilized RK           | 6     |
| Symplectic RK           | 18    |
| Taylor Series           | 2     |
| Tsit5                   | 1     |
| Verner                  | 4     |
| Core/Other              | 7     |

DiffEqDevTools.jl ODE Tableaus: 105

Total: 404
```

## What Solvers Are There?

For complete information, see:

https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/

And the new OrdinaryDiffEq API pages!

## What Solver Packages Are There?

There are many different packages to be aware of which all use the same API:

* OrdinaryDiffEq.jl: the main ODE solver package. Split into subpackages:
    * `OrdinaryDiffEqTsit5`: Just the non-stiff 5th order adaptive Tsit5 method
    * `OrdinaryDiffEqVerner`: The `VernX` high efficiency non-stiff methods
    * `OrdinaryDiffEqRosenbrock`: The Rosenbrock methods, i.e. `Rosenbrock23` and `Rodas5P`, for small stiff systems
    * `OrdinaryDiffEqBDF`: The BDF methods, `QNDF` and `FBDF`, for large stiff systems
    * `OrdinaryDiffEqDefault`: The default solver for automatic choice
* Sundials.jl: Wrapper for SUNDIALS `CVODE` and `IDA`. Can be efficient for large stiff systems
* LSODA.jl: Wrapper for the classic `lsoda`, tends to be generally good for small systems
* ODEInterfaceDiffEq.jl: Wrappers for classic Fortran ODE solvers, including `dopri5` and `radau`

## Stiff Equations

One major class of equations to know about are stiff equations. While difficult to
define rigorously, there are two simple way to think about them:

1. If your problem has multiple time scales (6+ orders of magnitude apart), then it's stiff. Look for parameter values that are very different.
2. If solvers for non-stiff equations are taking lots of steps, it's stiff.

This is a very common numerical difficulty, and when identified these problems require
different sets of solvers

## Stiff Equation Example: ROBER

```{julia}
#| echo: true
function rober!(du, u, p, t)
    y₁, y₂, y₃ = u
    k₁, k₂, k₃ = p
    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃
    du[2] = k₁ * y₁ - k₂ * y₂^2 - k₃ * y₂ * y₃
    du[3] = k₂ * y₂^2
    nothing
end

prob = DE.ODEProblem(rober!, [1.0, 0.0, 0.0], (0.0, 1e5), [0.04, 3e7, 1e4])
sol = DE.solve(prob, DE.Rodas5P())
Plots.plot(sol, tspan = (1e-6, 1e5))
```

## Stiff Equation Example: ROBER

```{julia}
#| echo: true
Plots.plot(sol, tspan = (1e-6, 1e5), xscale = :log10, yscale=:log10)
```

## Some special solvers you should be aware of

* `ROCK2`: An explicit method that is efficient for stiff equations which are dominated by real eigenvalues
* SSP methods: Methods which enforce certain properties (total variation, maximum norm, entropy) if dt is sufficiently small. This can be required for stability with some partial differential equations (hyperbolic equations)
* low-memory RK methods: These methods require less RAM than other methods, at the cost of being less computationally efficient. Good for very large PDE discretizations

## Faster Loading Times / Decreased Depedendency Usage

DifferentialEquations.jl loads a full set of solvers, but if you know exactly the
solver that you want, you can simply load the subpackage with that solver!

```{julia}
#| echo: true
import OrdinaryDiffEqTsit5 as ODE5

function lorenz!(du, u, p, t)
    x, y, z = u
    σ, ρ, β = p
    du[1] = dx = σ * (y - x)
    du[2] = dy = x * (ρ - z) - y
    du[3] = dz = x * y - β * z
end

u0 = [1.0, 0.0, 0.0]
tspan = (0.0, 100.0)
p = [10.0, 28.0, 8 / 3]
prob = ODE5.ODEProblem(lorenz!, u0, tspan, p)
sol = ODE5.solve(prob, ODE5.Tsit5())
Plots.plot(sol, idxs=(1,2,3))
```

## SplitODEProblem and IMEX methods

Sometimes you may have one part of the problem operating at a much faster time
scale than the other. In that case, you can split the problem and use a method
for stiff equations on the fast part and a method for explicit integrators on
the slow part. This is calls an implicit-explicit or IMEX integration. If we
define:

$$
u' = f(u,p,t) + g(u,p,t)
$$

## Example Defining A SplitODEProblem

```{julia}
#| echo: true
u = rand(4, 2)
f1 = (du, u, p, t) -> du .= 2u
f2 = (du, u, p, t) -> du .= 2u
prob = DE.SplitODEProblem(f1, f2, u, (0.0, 1.0))
sol = DE.solve(prob, DE.KenCarp4());
```

NOTE: SplitODEProblems can be solved by standard ODE solvers (by definition, just
putting them back together) but this affords no performance advantage, it's simply
a convenience.

## Special Case: Semilinear ODE Problem

There is a special case for a split ODE problem where $f(u,p,t)$ is linear, i.e.
$f(u,p,t)=Au$. When this occurs, ODE solvers can specialize on being able to solve
that part exactly via the matrix exponential $exp(At)v$, and thus we can use
speical integrators known as Exponential Runge-Kutta Methods. These can be fast
for PDE discretizations that tend of have this form.

```{julia}
#| echo: true
using OrdinaryDiffEqExponentialRK, SciMLOperators
A = [2.0 -1.0; -1.0 2.0]
linnonlin_f1 = MatrixOperator(A)
linnonlin_f2 = (du, u, p, t) -> du .= 1.01 .* u
linnonlin_fun_iip = SplitFunction(linnonlin_f1, linnonlin_f2)
tspan = (0.0, 1.0)
u0 = [0.1, 0.1]
prob = SplitODEProblem(linnonlin_fun_iip, u0, tspan)
sol = solve(prob, ETDRK4(), dt = 1 / 4)
```

## Linear Specialized Methods

Similarly, if $u'=f$ is almost linear, there are specializations. 

* For $u'=Au$, the analytical solution is $u(t) = exp(At)u_0$ which has fast solvers in ExponentialUtilities.jl. 
* For $u'=A(t)u$, there are Magnus methods
* For $u'=A(u)u$, there are Lie group methods

By using matrix exponentials, these methods can have good conservation properties.

## Example Magnus Method Solve

```{julia}
#| echo: true
using OrdinaryDiffEqLinear, SciMLOperators
function update_func(A, u, p, t)
    A[1, 1] = 0
    A[2, 1] = sin(t)
    A[1, 2] = -1
    A[2, 2] = 0
end
A0 = ones(2, 2)
A = MatrixOperator(A0, update_func! = update_func)
u0 = ones(2)
tspan = (0.0, 30.0)
prob = ODEProblem(A, u0, tspan)
sol = solve(prob, MagnusGL6(), dt = 1 / 4)
```

## DyanmicalODEProblem and Symplectic Integrators

```{julia}
#| echo: true
using OrdinaryDiffEqSymplecticRK, LinearAlgebra, ForwardDiff, Plots; gr()
H(q,p) = norm(p)^2/2 - inv(norm(q))
L(q,p) = q[1]*p[2] - p[1]*q[2]

pdot(dp,p,q,params,t) = ForwardDiff.gradient!(dp, q->-H(q, p), q)
qdot(dq,p,q,params,t) = ForwardDiff.gradient!(dq, p-> H(q, p), p)

initial_position = [.4, 0]
initial_velocity = [0., 2.]
initial_cond = (initial_position, initial_velocity)
initial_first_integrals = (H(initial_cond...), L(initial_cond...))
tspan = (0,100.)
prob = DynamicalODEProblem(pdot, qdot, initial_velocity, initial_position, tspan)
sol = solve(prob, KahanLi6(), dt=1//10);
```

## Symplectic Integrator Long Time Solution

```{julia}
#| echo: true
plot_orbit(sol) = plot(sol,idxs=(3,4), lab="Orbit", title="Kepler Problem Solution")

function plot_first_integrals(sol, H, L)
    plot(initial_first_integrals[1].-map(u->H(u.x[2], u.x[1]), sol.u), lab="Energy variation", title="First Integrals")
    plot!(initial_first_integrals[2].-map(u->L(u.x[2], u.x[1]), sol.u), lab="Angular momentum variation")
end
analysis_plot(sol, H, L) = plot(plot_orbit(sol), plot_first_integrals(sol, H, L))
analysis_plot(sol, H, L)
```

## Normal Integrator Long Time Solution

```{julia}
#| echo: true
sol = solve(prob, DE.Tsit5());
analysis_plot(sol, H, L)
```

## Mass Matrices and Differential-Algebraic Equations (DAEs)

Instead of just an ODE $u'=f(u,p,t)$, DifferentialEquations.jl can express mass
matrix ODEs:

$$
Mu' = f(u,p,t)
$$

In many cases this can be a performance improvement if $M$ is sparse (since $M^{-1}$)
would likely be dense! However, it can be more than just a performance improvement.
Mass matrices can be used to impose constraints when $M$ is singular.

This constrained ODE is called a DAE.

## DAE via Mass Matrix Example

Say we want to solve:

$$
\begin{aligned}
\frac{dy_1}{dt} &= -0.04 y_1 + 10^4 y_2 y_3 \\
\frac{dy_2}{dt} &=  0.04 y_1 - 10^4 y_2 y_3 - 3*10^7 y_{2}^2 \\
1 &=  y_{1} + y_{2} + y_{3} \\
\end{aligned}
$$

Let's write this in mass matrix form:

$$
\begin{align}
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0
\end{bmatrix} \begin{bmatrix}
y_1'\\
y_2'\\
y_3'
\end{bmatrix} = \begin{bmatrix}
-0.04 y_1 + 10^4 y_2 y_3\\
0.04 y_1 - 10^4 y_2 y_3 - 3*10^7 y_{2}^2\\
y_{1} + y_{2} + y_{3} - 1
\end{bmatrix} 
\end{align}
$$

If you do the matrix-vector multiplication out, you see that last row of zeros simply gives that the last equation is $0 = y_{1} + y_{2} + y_{3} - 1$. Once you see that trick, it's immediately clear how mass matrices can write out any constraint equation $g$. Done.

## DAE via Mass Matrix Example

```{julia}
#| echo: true
function rober(du, u, p, t)
    y₁, y₂, y₃ = u
    k₁, k₂, k₃ = p
    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃
    du[2] = k₁ * y₁ - k₃ * y₂ * y₃ - k₂ * y₂^2
    du[3] = y₁ + y₂ + y₃ - 1
    nothing
end
M = [1.0 0 0
     0 1.0 0
     0 0 0]
mmf = DE.ODEFunction(rober, mass_matrix = M)
prob_mm = DE.ODEProblem(mmf, [1.0, 0.0, 0.0], (0.0, 1e5), (0.04, 3e7, 1e4))
sol = DE.solve(prob_mm, DE.Rodas5(), reltol = 1e-8, abstol = 1e-8)

Plots.plot(sol, xscale = :log10, tspan = (1e-6, 1e5), layout = (3, 1))
```

## DAEs More Information

For a deeper dive into methods for defining DAEs, see the blog post:

https://www.stochasticlifestyle.com/machine-learning-with-hard-constraints-neural-differential-algebraic-equations-daes-as-a-general-formalism/

<!-- Local Variables: -->
<!-- mode: markdown -->
<!-- End: -->
